# -*- coding: utf-8 -*-
"""Untitled2_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bDmsIKeSgsrv9oBbITjnJdB0rQKPFG1E

#import libararies
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import seaborn as sns
from sklearn.tree import DecisionTreeRegressor,export_graphviz
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler
import graphviz

"""#read csv """

data = pd.read_csv("CustomersDataset (2).csv")

"""Description Data"""

data.describe(include="all")

"""Information of Data"""

print(data.info())

"""# convert to numirecal data"""

data['gender'] = pd.factorize(data.gender)[0]  # 0=>Female     1=>Male
data['Dependents'] = pd.factorize(data.Dependents)[0]  # 0=>no   1=>yes
data['Partner'] = pd.factorize(data.Partner)[0]  # 0=>yes   1=>no
data['PhoneService'] = pd.factorize(data.PhoneService)[0]  # 0=>no   1=>yes
data['MultipleLines'] = pd.factorize(data.MultipleLines)[0]  # 0=>no   1=>yes
data['OnlineSecurity'] = pd.factorize(data.OnlineSecurity)[0]  # 0=>no   1=>yes
data['OnlineBackup'] = pd.factorize(data.OnlineBackup)[0]  # 0=>yes   1=>no
data['DeviceProtection'] = pd.factorize(data.DeviceProtection)[0]  # 0=>no   1=>yes
data['TechSupport'] = pd.factorize(data.TechSupport)[0]  # 0=>no   1=>yes
data['StreamingTV'] = pd.factorize(data.StreamingTV)[0]  # 0=>no   1=>yes
data['StreamingMovies'] = pd.factorize(data.StreamingMovies)[0]  # 0=>no   1=>yes
data['PaperlessBilling'] = pd.factorize(data.PaperlessBilling)[0]  # 0=>yes  1=>no
data['InternetService'] = pd.factorize(data.InternetService)[0]  # 0=>DSL  1=>Fiber   2=>No
data['Contract'] = pd.factorize(data.Contract)[0]  # 0=>month_to_one      1=>one year      2=>two year
data['PaymentMethod'] = pd.factorize(data.PaymentMethod)[0]  # 0=>electronic   1=>Mailed   2=>Bank  3=>credit card
data['Churn'] = pd.factorize(data.Churn)[0]  # 0=>No  1=>Yes

"""# delete customer id unwanted feature

"""

data.drop('customerID', axis=1, inplace=True)

"""# find and drop null values
#print("before\n")
drop null
"""

print("before drop null\n")
print(data.shape)
data.dropna(inplace=True)

"""#print("after\n")
#print(data.shape)
#print(data.isnull().sum())
"""

print("after drop null\n")
print(data.shape)
print("\n")
print("data sum null")
print(data.isnull().sum())

"""# change data type from object to float of total charge
 
"""

print(data.dtypes)
data['TotalCharges'] = data['TotalCharges'].astype(float)
print(data['TotalCharges'].dtypes)
print("#####################################")

"""# remove outliers

"""

def outliers(df, ft):
    q1 = df[ft].quantile(0.25)
    q3 = df[ft].quantile(0.75)
    iqr = q3 - q1
    lower = q1 - (1.5) * iqr
    upper = q3 + (1.5) * iqr
    ls = df.index[(df[ft] < lower) | (df[ft] > upper)]
    return ls

index_list = []
for feature in ['Dependents', 'PhoneService', 'MultipleLines', 'OnlineSecurity', 'OnlineBackup',
                'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'PaperlessBilling',
                'InternetService', 'PaymentMethod', 'Churn', 'MonthlyCharges', 'tenure','gender','Partner', 'SeniorCitizen', 'Contract',
                'TotalCharges']:#, 'tenure''gender',, 'Partner', 'SeniorCitizen', 'Contract'
 index_list.extend(outliers(data, feature))
 
print(index_list)

def remove(df, ls):
    ls = sorted(set(ls))
    df = df.drop(ls)
    return df
print(data.shape)

data_new = remove(data, index_list)

print("after remove outliers")
print(data_new.shape)

"""#randomForest"""

x_=data_new[['gender','SeniorCitizen','Partner','Dependents','tenure','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport' ,'StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod','MonthlyCharges','TotalCharges']].values
y_=data_new['Churn'].values
X_train, X_test, y_train, y_test = train_test_split(x_,y_ , test_size=0.3)
model=RandomForestRegressor(n_estimators=100)
model.fit(X_train,y_train)
Predictions=model.predict(X_test)
print(model.feature_importances_)
for index in range(len(Predictions)):
  print('Actual:',y_[index],'Predications', Predictions[index])  
plt.scatter(y_test,Predictions,color="r")
plt.xlabel="Y_text"
plt.ylabel="Predications"
plt.show()

"""desion tree"""

x_=data_new[['gender','SeniorCitizen','Partner','Dependents','tenure','PhoneService','MultipleLines','InternetService','OnlineSecurity','OnlineBackup','DeviceProtection','TechSupport' ,'StreamingTV','StreamingMovies','Contract','PaperlessBilling','PaymentMethod','MonthlyCharges','TotalCharges']].values
y_=data_new['Churn'].values
X_train, X_test, y_train, y_test = train_test_split(x_,y_ , test_size=0.3)
model=DecisionTreeRegressor()
model.fit(X_train,y_train)
Predictions=model.predict(X_test)
print(model.feature_importances_)
for index in range(len(Predictions)):
  print('Actual:',y_[index],'Predications', Predictions[index])  
export_graphviz(model,out_file='DesisionTree.dot')
with open('DesisionTree.dot') as f:
  dot_graph=f.read()
g=graphviz.Source(dot_graph)
g.render()
print('PDF created')
plt.scatter(y_test,Predictions,color="b")
plt.xlabel="y_test"
plt.ylabel="Predictions"

plt.show()

import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt')
import nltk
nltk.download('wordnet')
import nltk
nltk.download('vader_lexicon')

"""Text Mining

"""

# ############################################################################################
# ##-------------------------------- text Mining --------------------------------------------
# #load data
with open("/content/text_data.txt") as f:
     lines = f.readlines()

print(lines) 
# #-----------------------------------------------------------

# #1st way to convert list to string
data = ' '.join(map(str,lines))
# #2nd way to convert list to string
# #data = ' '.join([str(l) for l in lines])

print(data)
# #-----------------------------------------------------------

# #Lowering
input_str=data
input_str=input_str.lower()

print(input_str)
# #-----------------------------------------------------------

# #Remove punctuation
import string
result = input_str.translate(str.maketrans('','',string.punctuation))
print("Remove punctuation")
print(result)
# # The .maketrans() method here takes three arguments, the first two of which are empty strings,
# # and the third is the list of punctuation we want to remove. This tells the function to replace
# # all punctuation with None
# # In case you’re curious what punctuation are included in the string.punctuation, let’s have a quick look:
# #-----------------------------------------------------------

# #Removeing Stopwords and  then Tokenization
input_str = result
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
stop_words = set(stopwords.words('english'))
tokens = word_tokenize(input_str)
result = [i for i in tokens if not i in stop_words]
print("Removeing Stopwords and  then Tokenization")
print(result)
# #-----------------------------------------------------------

# #Stemming after tokenization
from nltk.stem import PorterStemmer
stemmer= PorterStemmer()
print("Stemming after tokenization")
for word in result:
   print(stemmer.stem(word))
# #-----------------------------------------------------------

# #Lemmatization after tokenization, Stemming
from nltk import WordNetLemmatizer
lemmatizer=WordNetLemmatizer()
input_str=result
print("Lemmatization after tokenization, Stemming")
for word in input_str:
    print(lemmatizer.lemmatize(word))
# #-----------------------------------------------------------
# #Sentiment analysis in Python
from nltk.sentiment import SentimentIntensityAnalyzer
sia=SentimentIntensityAnalyzer()
print("Sentiment analysis in Python")
sia.polarity_scores(data)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from numpy import where, quantile, random, ndarray
from sklearn.datasets import make_blobs
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.neighbors import LocalOutlierFactor

sub_data = data_new.head(50)
print("First 50 row")
print(sub_data)

"""Visualize """

# 2. BoxPlot
plt.boxplot(sub_data.MonthlyCharges, notch=True, vert=False)
plt.show()

# 3. Donut Chart
plt.pie(sub_data['MonthlyCharges'])
# Creating a Circle
circle = plt.Circle((0, 0), 0.7, color='white')
p = plt.gcf()
p.gca().add_artist(circle)
plt.show()

# histplot
sns.histplot(x='InternetService', data=sub_data, kde=True, hue='gender')
plt.show()

sns.histplot(x='PaymentMethod', data=sub_data, kde=True, hue='gender')
plt.show()

sns.histplot(x='Contract', data=sub_data, kde=True, hue='gender')
plt.show()

"""Outliers Detect

"""

data = pd.read_csv('/content/CustomersDataset (2).csv')
sub_data = data.head(50)
print(sub_data)

# 7. Apply anomaly detection technique to find outliers
from sklearn.cluster import DBSCAN
random.seed(7)
x = sub_data[['tenure', 'MonthlyCharges']].values
print(x)
plt.scatter(x[:, 0], x[:, 1])
plt.show()
lof = LocalOutlierFactor(n_neighbors=20, contamination=.03)
y_pred = lof.fit_predict(x)
lofs_index = where(y_pred == -1)
values = x[lofs_index]
plt.scatter(x[:, 0], x[:, 1])
plt.scatter(values[:, 0], values[:, 1], color='r')
plt.show()